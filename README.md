# 🧠 ShrunkIQ
Smart Readability Evaluation of Compressed Documents using LLMs

ShrunkIQ is a research-driven framework that evaluates how well compressed documents retain their meaning and readability — using powerful language models. It helps you quantify whether compression artifacts (especially in PDF/image-based documents) affect downstream tasks like question answering or semantic understanding.

# 🚀 Why ShrunkIQ?
* 🗜️ Traditional compression tools reduce file size — but at what cognitive cost?
* 📉 Low quality may distort text and structure, breaking comprehension.
* 🧠 LLMs can “reconstruct” meaning — but this may mask real quality loss.
* 🔍 ShrunkIQ offers a transparent and measurable way to evaluate this tradeoff.

# 🔒 Trust, Don't Hallucinate
LLMs are smart — but their prior knowledge may fill in missing text.
ShrunkIQ tackles this challenge by:

* 👁️ Mimicking Human Perception:
During evaluation, AI only sees what human would see or interpret a visually degraded document (no bias, no assumptions and no guessing).

* 🧠 Answering Only What’s There:
During evaluation, LLMs are instructed to answer only based on what human sees, not prior world knowledge.

* ✅ Prioritizing Critical Information:
The scoring system helps ensure the most semantically important content survives compression.




# 📘 Use Cases
* 🧪 Evaluating OCR Pipeline Robustness

* 📉 Benchmarking Compression Algorithms on Real Tasks

* 🎯 Unbiased Evaluation of Semantic Preservation

* 📄 Ensuring Fidelity in Legal, Academic, and Financial Documents



# 🧩 How It Works
[WIP]

# 📦 Installation
[WIP]