# ğŸ§  ShrunkIQ
Smart Readability Evaluation of Compressed Documents using LLMs

ShrunkIQ is a research-driven framework that evaluates how well compressed documents retain their meaning and readability â€” using powerful language models. It helps you quantify whether compression artifacts (especially in PDF/image-based documents) affect downstream tasks like question answering or semantic understanding.

# ğŸš€ Why ShrunkIQ?
* ğŸ—œï¸ Traditional compression tools reduce file size â€” but at what cognitive cost?
* ğŸ“‰ Low quality may distort text and structure, breaking comprehension.
* ğŸ§  LLMs can â€œreconstructâ€ meaning â€” but this may mask real quality loss.
* ğŸ” ShrunkIQ offers a transparent and measurable way to evaluate this tradeoff.

# ğŸ”’ Trust, Don't Hallucinate
LLMs are smart â€” but their prior knowledge may fill in missing text.
ShrunkIQ tackles this challenge by:

* ğŸ‘ï¸ Mimicking Human Perception:
During evaluation, AI only sees what human would see or interpret a visually degraded document (no bias, no assumptions and no guessing).

* ğŸ§  Answering Only Whatâ€™s There:
During evaluation, LLMs are instructed to answer only based on what human sees, not prior world knowledge.

* âœ… Prioritizing Critical Information:
The scoring system helps ensure the most semantically important content survives compression.




# ğŸ“˜ Use Cases
* ğŸ§ª Evaluating OCR Pipeline Robustness

* ğŸ“‰ Benchmarking Compression Algorithms on Real Tasks

* ğŸ¯ Unbiased Evaluation of Semantic Preservation

* ğŸ“„ Ensuring Fidelity in Legal, Academic, and Financial Documents



# ğŸ§© How It Works
[WIP]

# ğŸ“¦ Installation
[WIP]